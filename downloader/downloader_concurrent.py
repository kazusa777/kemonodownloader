import logging

log = logging.getLogger(__name__)
from pathlib import Path
import os, aiohttp, aiofiles, asyncio
from scraper.kemono import fetch_and_yield_post, extract_attachments_urls_by_id


# --- ä¸‹è½½å›¾ç‰‡ ---
async def download_image(session, url, save_path, sem, pause_event=None, stop_event=None, logger=None, error_signal=None):
    from pathlib import Path
    temp_path = save_path + ".temp"
    async with sem:
        for attempt in range(1, 4):
            if stop_event and stop_event.is_set():
                if logger: logger("â›” åµæ¸¬åˆ°çµ‚æ­¢ï¼Œé€€å‡ºåœ–ç‰‡ä¸‹è¼‰")
                return
            while pause_event and not pause_event.is_set():
                await asyncio.sleep(0.2)
            try:
                async with session.get(url) as resp:
                    if resp.status == 200:
                        async with aiofiles.open(temp_path, "wb") as f:
                            await f.write(await resp.read())
                        os.replace(temp_path, save_path)
                        msg = f"âœ… åœ–ç‰‡ä¸‹è¼‰å®Œæˆ: {Path(save_path).as_posix()}"
                        if logger: logger(msg)
                        return
                    else:
                        raise Exception(f"{resp.status}")
            except Exception as e:
                if attempt < 3:
                    if logger: logger(f"âš ï¸ åœ–ç‰‡ä¸‹è¼‰å¤±æ•—ï¼ˆç¬¬ {attempt} æ¬¡ï¼‰ï¼Œé‡è©¦ä¸­: {url}")
                    await asyncio.sleep(1)
                else:
                    msg = f"âŒ åœ–ç‰‡ä¸‹è¼‰å¤±æ•—ï¼ˆå·²é‡è©¦ 3 æ¬¡ï¼‰: {url} - {e}"
                    if logger: logger(msg)
                    await record_error(save_path, error_signal=error_signal, url=url)


# --- ä¸‹è½½å¤§æ–‡ä»¶ ---
async def download_with_resume(session, url, save_path, sem, chunk_size=1024 * 1024, pause_event=None, stop_event=None,
                               logger=None, error_signal=None):
    from pathlib import Path
    temp_path = save_path + ".temp"
    async with sem:
        for attempt in range(1, 4):
            if stop_event and stop_event.is_set():
                if logger: logger("â›” åµæ¸¬åˆ°çµ‚æ­¢ï¼Œé€€å‡ºå¤§æ–‡ä»¶ä¸‹è¼‰")
                return
            while pause_event and not pause_event.is_set():
                await asyncio.sleep(0.2)
            try:
                file_size = os.path.getsize(temp_path) if os.path.exists(temp_path) else 0
                headers = {"Range": f"bytes={file_size}-"} if file_size else {}
                async with session.get(url, headers=headers) as resp:
                    if resp.status not in (200, 206):
                        raise Exception(f"{resp.status}")
                    mode = "ab" if file_size else "wb"
                    async with aiofiles.open(temp_path, mode) as f:
                        async for chunk in resp.content.iter_chunked(chunk_size):
                            if stop_event and stop_event.is_set():
                                if logger: logger("â›” åµæ¸¬åˆ°çµ‚æ­¢ï¼ˆå¤§æ–‡ä»¶å€å¡Šï¼‰")
                                return
                            while pause_event and not pause_event.is_set():
                                await asyncio.sleep(0.2)
                            await f.write(chunk)
                    total = int(resp.headers.get("Content-Range", "").split("/")[-1]
                                or resp.headers.get("Content-Length", 0) or 0)
                    final_size = os.path.getsize(temp_path)
                    if total and final_size >= total:
                        os.replace(temp_path, save_path)
                        if logger: logger(f"âœ… å¤§æ–‡ä»¶ä¸‹è½½å®Œæˆ: {Path(save_path).as_posix()}")
                    else:
                        if logger: logger(f"âš ï¸ æ–‡ä»¶éƒ¨åˆ†ä¿å­˜: {Path(temp_path).as_posix()}")
                    return
            except Exception as e:
                if attempt < 3:
                    if logger: logger(f"âš ï¸ å¤§æ–‡ä»¶ä¸‹è¼‰å¤±æ•—ï¼ˆç¬¬ {attempt} æ¬¡ï¼‰ï¼Œé‡è©¦ä¸­: {url}")
                    await asyncio.sleep(1)
                else:
                    if logger: logger(f"âŒ å¤§æ–‡ä»¶ä¸‹è¼‰å¤±æ•—ï¼ˆå·²é‡è©¦ 3 æ¬¡ï¼‰: {url} - {e}")
                    await record_error(save_path, error_signal=error_signal, url=url)


async def download_worker(
        task_queue, sem, session,
        pause_event, stop_event, update_status, logger,
        file_progress_signal=None,
        post_file_counter=None,
        post_file_done=None,
        cancelled_posts=None,
        error_signal=None
):
    """
    å–®ä¸€æª”æ¡ˆä¸‹è¼‰ workerï¼Œä¸æ–·å¾ä»»å‹™ä½‡åˆ—ä¸­æ‹‰å– FileTask é€²è¡Œè™•ç†ã€‚
    æ”¯æ´æš«åœã€çµ‚æ­¢æ§åˆ¶ã€‚

    åƒæ•¸ï¼š
        task_queue (asyncio.Queue): ä¸‹è¼‰ä»»å‹™ä½‡åˆ—
        sem (asyncio.Semaphore): ä¸¦ç™¼ä¸‹è¼‰æ§åˆ¶ä¿¡è™Ÿé‡
        session (aiohttp.ClientSession): aiohttp æœƒè©±
        pause_event / stop_event: æ§åˆ¶æ¨™èªŒ
        update_status (Callable): ç‹€æ…‹æ›´æ–°å›å‘¼
        logger (Callable): æ—¥èªŒå‡½å¼
    """
    file_done = 0
    while True:
        try:
            task = await task_queue.get()

            if stop_event and stop_event.is_set():
                if logger: logger("â›” çµ‚æ­¢æŒ‡ä»¤ä¸‹é”ï¼Œworker åœæ­¢")
                break

            while pause_event and not pause_event.is_set():
                await asyncio.sleep(0.3)

            if os.path.exists(task.save_path):
                if logger: logger(f"â­ï¸ å·²å­˜åœ¨ï¼Œè·³é: {task.save_path}")
                if file_progress_signal:
                    file_progress_signal.emit(1)
                task_queue.task_done()
                continue

            async with sem:
                # é€‰æ‹©ä¸‹è½½æ–¹å¼
                if task.file_type == "image":
                    await download_image(session, task.url, task.save_path, sem, pause_event, stop_event, logger,error_signal)
                else:
                    await download_with_resume(session, task.url, task.save_path, sem,
                                               pause_event=pause_event,
                                               stop_event=stop_event,
                                               logger=logger,
                                               error_signal=error_signal)

                if update_status:
                    await update_status(task.user_id, task.post_id, finished=False)  # å¯è®¾ç½®ä¸º per-file çŠ¶æ€

            file_done += 1
            if file_progress_signal:
                file_progress_signal.emit(1)
                # print("å®Œæˆæ•°+1")
            task_queue.task_done()

            key = (task.user_id, task.post_id)
            post_file_done[key] = post_file_done.get(key, 0) + 1

            if post_file_done[key] == post_file_counter.get(key, 0):
                if cancelled_posts and key in cancelled_posts:
                    if logger:
                        logger(f"â›” å·²å–æ¶ˆä»»å‹™ï¼Œä¸æ¨™è¨˜ç‚ºå®Œæˆï¼š{key}")
                elif stop_event and stop_event.is_set():
                    if logger:
                        logger(f"â›” åµæ¸¬åˆ°çµ‚æ­¢æ¨™èªŒï¼Œè·³éæ¨™è¨˜å®Œæˆï¼š{key}")
                else:
                    if update_status:
                        await update_status(task.user_id, task.post_id, finished=True)
        except Exception as e:
            if logger:
                logger(f"âŒ ä¸‹è¼‰ä»»å‹™ç•°å¸¸: {e}")
            # record_error(task.save_path, error_signal=error_signal, url=task.url)
            task_queue.task_done()


# --- ä¸‹è¼‰ä¸»å¾ªç’° ---
async def download_streamed_posts(
        url=None,
        post_stream=None,
        base_path=None,
        concurrency=10,
        update_status=None,
        pause_event=None,
        stop_event=None,
        logger=None,
        day_mode=0,
        selected_ids=None,
        signal_host=None,
        error_signal=None
):
    """
    ä»¥æª”æ¡ˆç‚ºå–®ä½é€²è¡Œä¸‹è¼‰ï¼Œä½¿ç”¨ä»»å‹™ä½‡åˆ—é©…å‹•ä¸¦è¡Œ workerã€‚
    è²¼æ–‡é€²åº¦èˆ‡æª”æ¡ˆé€²åº¦çš†å³æ™‚è¨Šè™Ÿå‚³éã€‚
    """
    cancelled_posts = set()

    post_total = 0
    post_done = 0
    file_total = 0
    file_done = 0
    task_queue = asyncio.Queue()
    sem = asyncio.Semaphore(concurrency)
    post_file_counter = {}
    post_file_done = {}

    file_max_signal = getattr(signal_host, "file_max_signal", None)
    file_progress_signal = getattr(signal_host, "file_progress_signal", None)
    post_max_signal = getattr(signal_host, "post_max_signal", None)
    post_progress_signal = getattr(signal_host, "post_progress_signal", None)

    async with aiohttp.ClientSession() as session:
        workers = [
            asyncio.create_task(
                download_worker(
                    task_queue, sem, session,
                    pause_event, stop_event, update_status, logger,
                    file_progress_signal=file_progress_signal,
                    post_file_counter=post_file_counter,
                    post_file_done=post_file_done,
                    cancelled_posts=cancelled_posts,
                    error_signal=error_signal
                )
            )
            for _ in range(concurrency)
        ]

        fetch_sem = asyncio.Semaphore(5)

        if post_stream is None and selected_ids is not None:
            post_stream = extract_attachments_urls_by_id(url, selected_ids=selected_ids, day_mode=day_mode)

        if post_stream is not None:
            if isinstance(post_stream, list):
                async def wrap_list():
                    for item in post_stream:
                        yield item

                post_stream = wrap_list()
            if logger: logger("ğŸ“¦ ä½¿ç”¨ post_stream ä¸‹è¼‰")

            async for post in post_stream:
                if stop_event and stop_event.is_set():
                    key = (post["user"], post["id"])
                    cancelled_posts.add(key)
                    if logger: logger(f"â›” æª¢æ¸¬åˆ°çµ‚æ­¢ï¼Œä¸­æ–·è²¼æ–‡ï¼š{post.get('title')}ï¼ˆ{key}ï¼‰")
                    break

                while pause_event and not pause_event.is_set():
                    await asyncio.sleep(0.2)

                post_total += 1
                if post_max_signal:
                    post_max_signal.emit(post_total)

                def count_file(n):
                    nonlocal file_total
                    file_total += n
                    if file_max_signal:
                        file_max_signal.emit(n)

                await fetch_and_yield_post(
                    post, session, fetch_sem, day_mode, logger,
                    task_queue, base_path, on_new_file=count_file,
                    post_file_counter=post_file_counter,
                    signal_host=signal_host
                )

                post_done += 1
                if post_progress_signal:
                    post_progress_signal.emit(post_done)
        else:
            if logger: logger("âŒ ç„¡ä¸‹è¼‰æºï¼ˆpost_stream å’Œ selected_ids å‡ç‚ºç©ºï¼‰")

        await task_queue.join()
        for w in workers:
            w.cancel()
        await asyncio.gather(*workers, return_exceptions=True)

        if logger:
            logger("âœ… å…¨éƒ¨æ–‡ä»¶ä¸‹è¼‰å®Œæˆ")


def record_error(path, error_signal=None, url=None):
    try:
        posix_path = Path(path).as_posix()

        log_path = os.path.join(os.path.dirname(path), "error.txt")
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"{posix_path} | {url or 'N/A'}\n")
        if error_signal:
            error_signal.emit([f"{posix_path} | {url or 'N/A'}"])
    except Exception as e:
        print(f"âŒ å¯«å…¥ error.txt å¤±æ•—: {e}")
